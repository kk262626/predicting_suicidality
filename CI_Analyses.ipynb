{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Analyses- Producing 95% CIs and Examining Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from matplotlib import pyplot \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import confusion_matrix as confusion\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapped CIs for First Round of Evals (Full Test Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading datasets\n",
    "Xi_hold=pd.read_csv(r'C:\\Users\\z5291979\\OneDrive - UNSW\\Documents\\lsac-data\\processed_data\\Xi_hold.csv')\n",
    "y_hold_si=pd.read_csv(r'C:\\Users\\z5291979\\OneDrive - UNSW\\Documents\\lsac-data\\processed_data\\y_hold_si.csv')\n",
    "y_hold_nssi=pd.read_csv(r'C:\\Users\\z5291979\\OneDrive - UNSW\\Documents\\lsac-data\\processed_data\\y_hold_nssi.csv')\n",
    "y_hold_att=pd.read_csv(r'C:\\Users\\z5291979\\OneDrive - UNSW\\Documents\\lsac-data\\processed_data\\y_hold_att.csv')\n",
    "y_hold_sitbs=pd.read_csv(r'C:\\Users\\z5291979\\OneDrive - UNSW\\Documents\\lsac-data\\processed_data\\y_hold_sitbs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unnamed: 0 is an extra index column\n",
    "Xi_hold=Xi_hold.drop(columns=['Unnamed: 0'])\n",
    "Xi_hold.to_csv(r'C:\\Users\\z5291979\\OneDrive - UNSW\\Documents\\lsac-data\\processed_data\\Xi_hold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to prepare 100 bootstrapped test sets, and obtain estimates of performance metrics for each resampled version of test data\n",
    "algos=['LR', 'RF', 'XGB']\n",
    "threshs1=[0.188868, 0.175579, 0.119263]\n",
    "models1= {a: joblib.load(f'{a}_si.sav') for a in algos}\n",
    "\n",
    "def boot(model, fulltest, thresh):\n",
    "    #Suppressing warnings for this section. Warning appears because the bootstrapped samples do not have feature names as they are arrays while the model has previously been fitted on pandas dataframes with feature names\n",
    "    #This is ok because the features are in the same order as the original test set\n",
    "    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "    n_iterations=100\n",
    "    aucs=list()\n",
    "    f1s=list()\n",
    "    sens=list()\n",
    "    specs=list()\n",
    "    ppvs=list()\n",
    "    values=fulltest.values\n",
    "    for i in range(n_iterations):\n",
    "        test=resample(values, n_samples=len(fulltest), stratify=values[:, -1], random_state=i)\n",
    "        probs=model.predict_proba(test[:, :-1])\n",
    "        probs=probs[:, 1]\n",
    "        auc=roc_auc_score(test[:, -1], probs)\n",
    "        aucs.append(auc)\n",
    "        pred=np.where(probs>= thresh, 1, 0)\n",
    "        f1=f1_score(test[:, -1], pred)\n",
    "        f1s.append(f1)\n",
    "        sen=recall(test[:, -1], pred)\n",
    "        sens.append(sen)\n",
    "        ppv=precision_score(test[:, -1], pred)\n",
    "        ppvs.append(ppv)\n",
    "        tn, fp, fn, tp=confusion(test[:, -1], pred).ravel()\n",
    "        spec=tn/(tn+fp)\n",
    "        specs.append(spec)\n",
    "        \n",
    "    metrics=[\"aucs\", \"f1s\", \"sens\", \"specs\", \"ppvs\"]\n",
    "    metricsdf=pd.DataFrame(zip(aucs, f1s, sens, specs, ppvs), columns=metrics)\n",
    "    return metricsdf\n",
    "\n",
    "#Iterating through tuples to evaluate LR, RF, and XGB Models \n",
    "#Value of the tuple algo is paired to the other corresponding value in threshs, i.e. 'LR' is paired with threshold of 0.188868\n",
    "def runevals(models1, fulltest, threshs):\n",
    "    for a, t in zip(algos, threshs):\n",
    "        clf=models1[a]\n",
    "        print(f'95% CIs for performance using {a} with threshold at {t}')\n",
    "        metricsdf=boot(clf, fulltest, t)\n",
    "        print(metricsdf.quantile([.025, .975]))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% CIs for performance using LR with threshold at 0.188868\n",
      "           aucs       f1s    sens     specs      ppvs\n",
      "0.025  0.694001  0.308598  0.3750  0.816746  0.262009\n",
      "0.975  0.786711  0.447163  0.5625  0.864968  0.367685\n",
      "\n",
      "\n",
      "95% CIs for performance using RF with threshold at 0.175579\n",
      "           aucs       f1s      sens     specs      ppvs\n",
      "0.025  0.777556  0.407189  0.624479  0.754305  0.295607\n",
      "0.975  0.865730  0.509746  0.781250  0.818404  0.385700\n",
      "\n",
      "\n",
      "95% CIs for performance using XGB with threshold at 0.119263\n",
      "           aucs       f1s      sens     specs      ppvs\n",
      "0.025  0.783426  0.385365  0.760417  0.654779  0.256433\n",
      "0.975  0.866916  0.467795  0.901302  0.723618  0.320392\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fulltest_si=Xi_hold.join(y_hold_si)\n",
    "runevals(models1, fulltest_si, threshs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% CIs for performance using LR with threshold at 0.138621\n",
      "           aucs       f1s      sens     specs      ppvs\n",
      "0.025  0.655818  0.224323  0.321429  0.855869  0.170443\n",
      "0.975  0.808005  0.385919  0.553571  0.900446  0.293034\n",
      "\n",
      "\n",
      "95% CIs for performance using RF with threshold at 0.160344\n",
      "           aucs       f1s      sens     specs      ppvs\n",
      "0.025  0.771589  0.315188  0.544196  0.821620  0.217391\n",
      "0.975  0.888752  0.445714  0.795089  0.870022  0.313881\n",
      "\n",
      "\n",
      "95% CIs for performance using XGB with threshold at 0.172887\n",
      "           aucs       f1s      sens     specs      ppvs\n",
      "0.025  0.769216  0.302383  0.391964  0.895988  0.248378\n",
      "0.975  0.879479  0.511278  0.670089  0.936107  0.437224\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshs2=[0.138621, 0.160344, 0.172887]\n",
    "models2= {a: joblib.load(f'{a}_nssi.sav') for a in algos}\n",
    "\n",
    "fulltest_nssi=Xi_hold.join(y_hold_nssi)\n",
    "runevals(models2, fulltest_nssi, threshs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% CIs for performance using LR with threshold at 0.074037\n",
      "           aucs       f1s      sens     specs      ppvs\n",
      "0.025  0.690694  0.126141  0.296094  0.819907  0.080938\n",
      "0.975  0.846409  0.267160  0.625000  0.869440  0.171988\n",
      "\n",
      "\n",
      "95% CIs for performance using RF with threshold at 0.073374\n",
      "           aucs       f1s      sens     specs     ppvs\n",
      "0.025  0.800614  0.174382  0.562500  0.764562  0.10436\n",
      "0.975  0.915505  0.280363  0.860156  0.817109  0.16923\n",
      "\n",
      "\n",
      "95% CIs for performance using XGB with threshold at 0.08742\n",
      "           aucs       f1s     sens     specs      ppvs\n",
      "0.025  0.795508  0.224842  0.43750  0.873745  0.151437\n",
      "0.975  0.910338  0.381892  0.78125  0.909003  0.254234\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshs3=[0.074037, 0.073374, 0.087420]\n",
    "models3= {a: joblib.load(f'{a}_att.sav') for a in algos}\n",
    "\n",
    "fulltest_att=Xi_hold.join(y_hold_att)\n",
    "runevals(models3, fulltest_att, threshs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% CIs for performance using LR with threshold at 0.208806\n",
      "           aucs       f1s      sens     specs      ppvs\n",
      "0.025  0.666220  0.379787  0.504132  0.750699  0.296542\n",
      "0.975  0.756427  0.477128  0.624174  0.821752  0.392540\n",
      "\n",
      "\n",
      "95% CIs for performance using RF with threshold at 0.15213\n",
      "           aucs       f1s      sens     specs      ppvs\n",
      "0.025  0.772562  0.431631  0.702479  0.672615  0.309099\n",
      "0.975  0.845087  0.521524  0.830785  0.735197  0.380550\n",
      "\n",
      "\n",
      "95% CIs for performance using XGB with threshold at 0.176893\n",
      "           aucs       f1s      sens     specs      ppvs\n",
      "0.025  0.781079  0.452331  0.632025  0.750781  0.346575\n",
      "0.975  0.851117  0.545517  0.772934  0.810855  0.429257\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshs4=[0.208806, 0.152130, 0.176893]\n",
    "models4= {a: joblib.load(f'{a}_sitbs.sav') for a in algos}\n",
    "\n",
    "fulltest_sitbs=Xi_hold.join(y_hold_sitbs)\n",
    "runevals(models4, fulltest_sitbs, threshs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapped CIs for Second Round Evals (Reduced Test Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the reduced datasets created in the evaluation notebook\n",
    "si_test=pd.read_csv(r'C:\\Users\\z5291979\\OneDrive - UNSW\\Documents\\lsac-data\\processed_data\\y_hold_si.csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
